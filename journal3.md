The question for reflection this week is about contrasting digital history work with "regular" history, which sets up an interesting comparison. As we see in the exercises for this week, which involve using digital methods to clean and organize data, the insights from digital history work are very unique but also dependent on how well we could clean messy data.

One key difference here I noticed was with verification. We worked on an archival document, the correspondence of the Republic of Texas, taking it through various steps to clean it and then map it. While everyone doing the exercise would have started with the same raw data, we would have ended up with different final products, depending on how we did all the steps. I was not able to whittle down the number of correspondents to the figures given by the professor, so I know that I missed something along the way or was not as thorough while using Open Refine.

Perhaps this isn't very different from the work of a regular historian, whose work relies on their research and the number of sources or texts they consulted to reach a conclusion. Our conclusion relies on how well we cleaned the data, how many typos we found, how many rows we manually corrected etc; But a regular historian's work would include detailed citations to source material. Our source material includes work done in Open Refine, Gephi and elsewhere. How do we accurately source all this?

The challenge I see here is verifying all this behind-the-scenes work. In data journalism, we often have data editors who oversee the work of data reporters. We also have reporters verify each others' work by trying to replicate the analysis independently. 

Even though everyone doing this week's exercise would have followed similar steps and used the same tools, they would reach different end documents. This means that digital history needs not just citation, but also replication - how close does my end products or conclusion or findings come to other people doing this analysis?